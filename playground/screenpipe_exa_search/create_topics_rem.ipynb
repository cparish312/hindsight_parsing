{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/connorparish/miniconda3/envs/hindsight_server/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import sqlite3\n",
    "import pytz\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/Users/connorparish/code/hindsight\")\n",
    "\n",
    "from hindsight_server.utils import ocr_results_to_str\n",
    "from hindsight_server.config import LLM_MODEL_NAME\n",
    "from hindsight_server.query.query import load, llm_generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_db_path = \"/Users/connorparish/Library/Containers/today.jason.rem/Data/Library/Application Support/today.jason.rem/db.sqlite3\"\n",
    "conn = sqlite3.connect(rem_db_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_time_range(time_range):\n",
    "    utc_time_range = list()\n",
    "    for i, time in enumerate(time_range):\n",
    "        alter_time = time.astimezone(pytz.UTC)\n",
    "        utc_time_range.append(alter_time.strftime('%Y-%m-%dT%H:%M:%S.%f'))\n",
    "    return utc_time_range\n",
    "\n",
    "yesterday = datetime.now() - timedelta(days=1)\n",
    "time_range = [yesterday, datetime.now()]\n",
    "utc_time_range = preprocess_time_range(time_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"SELECT f.id as frameId, f.timestamp, f.activeApplicationName, ft.id as textId, ft.text, ft.x, ft.y, ft.w, ft.h FROM frames as f JOIN frames_text as ft ON f.id = ft.frameId WHERE f.timestamp BETWEEN '{utc_time_range[0]}' AND '{utc_time_range[1]}'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "today_frames = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frame_ids = set(today_frames['frameId'])\n",
    "random_frame_ids = random.sample(list(all_frame_ids), 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/c_/2c9vmfhd35bgwc_6h0q80d180000gn/T/ipykernel_60584/3811660279.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  frames['conf'] = 1\n"
     ]
    }
   ],
   "source": [
    "frames = today_frames.loc[today_frames['frameId'].isin(random_frame_ids)]\n",
    "frames['conf'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = \"\"\n",
    "for frame_id in frames['frameId'].unique():\n",
    "    frame_df = frames.loc[frames['frameId'] == frame_id]\n",
    "    all_text += ocr_results_to_str(frame_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You are an assistant who generates search queries for an embedding search database to help a user find content of interest.\n",
    "\n",
    "Below is content that has been on the user's screen recently:\n",
    "\n",
    "{all_text}\n",
    "\n",
    "Task:\n",
    "    - Analyze the content above.\n",
    "    - Identify the main themes and topics.\n",
    "    - Generate a single, concise sentence that summarizes the user's interests.\n",
    "    - Include as many relevant keywords and concepts from the content as possible.\n",
    "    - The sentence should be suitable as a search query for finding similar content. \n",
    "\n",
    "Return a JSON object with the following structure:\n",
    "{{\n",
    "    \"query\": \"sentence for querying the embedding search database\",\n",
    "    \"highlights\": \"Quick list of highlights used to generate the query\",\n",
    "    \"quality\": \"float between 0 and 1 estimating the quality of the generated query for finding content of interest\"\n",
    "}}\n",
    "\n",
    "Rules:\n",
    "- Do not add backticks to the JSON eg \\`\\`\\`json\\`\\`\\` is WRONG\n",
    "- DO NOT RETURN ANYTHING BUT JSON. NO COMMENTS BELOW THE JSON.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pass to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "# from mlx_lm import load\n",
    "from outlines import generate, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM_MODEL_NAME = \"mlx-community/Llama-3.1-8B-Instruct\"\n",
    "# LLM_MODEL_NAME = \"mlx-community/Meta-Llama-3-8B-Instruct-8bit\"\n",
    "LLM_MODEL_NAME = \"mlx-community/Llama-3.2-3B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 7 files: 100%|██████████| 7/7 [00:00<00:00, 124936.71it/s]\n"
     ]
    }
   ],
   "source": [
    "# Define JSON schema\n",
    "json_schema = \"\"\"\n",
    "    {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"query\": {\"type\": \"string\"},\n",
    "        \"highlights\": {\"type\": \"string\"},\n",
    "        \"quality\" : {\"type\" : \"number\"}\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Load MLX model\n",
    "# model, tokenizer = load(LLM_MODEL_NAME)\n",
    "\n",
    "# Create Outlines model wrapper\n",
    "mlx_model = models.mlxlm(LLM_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = models.mlxlm(\"mlx-community/Meta-Llama-3.1-8B-Instruct-8bit\")\n",
    "# generator = generate.text(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer = generator(\"My name is Connor Parish and \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate JSON output\n",
    "generator = generate.json(mlx_model, json_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hindsight_server/lib/python3.10/site-packages/outlines/generate/api.py:503\u001b[0m, in \u001b[0;36mSequenceGeneratorAdapter.__call__\u001b[0;34m(self, prompts, max_tokens, stop_at, seed, **model_specific_params)\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_sequence(sequences)\n\u001b[1;32m    499\u001b[0m generation_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_generation_parameters(\n\u001b[1;32m    500\u001b[0m     max_tokens, stop_at, seed\n\u001b[1;32m    501\u001b[0m )\n\u001b[0;32m--> 503\u001b[0m completions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_specific_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m(completions)\n",
      "File \u001b[0;32m~/miniconda3/envs/hindsight_server/lib/python3.10/site-packages/outlines/models/mlxlm.py:41\u001b[0m, in \u001b[0;36mMLXLM.generate\u001b[0;34m(self, prompts, generation_parameters, logits_processor, sampling_parameters)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate\u001b[39m(\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     33\u001b[0m     prompts: Union[\u001b[38;5;28mstr\u001b[39m, List[\u001b[38;5;28mstr\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     sampling_parameters: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSamplingParameters\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     37\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m     38\u001b[0m     streamer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[1;32m     39\u001b[0m         prompts, generation_parameters, logits_processor, sampling_parameters\n\u001b[1;32m     40\u001b[0m     )\n\u001b[0;32m---> 41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/hindsight_server/lib/python3.10/site-packages/outlines/models/mlxlm.py:109\u001b[0m, in \u001b[0;36mMLXLM.stream\u001b[0;34m(self, prompts, generation_parameters, logits_processor, sampling_parameters)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# Adapted from\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# https://github.com/ml-explore/mlx-examples/blob/4872727/llms/mlx_lm/utils.py#L267\u001b[39;00m\n\u001b[1;32m    107\u001b[0m prompt_tokens \u001b[38;5;241m=\u001b[39m mx\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlx_tokenizer\u001b[38;5;241m.\u001b[39mencode(prompts))\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (token, prob), n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_step(prompt_tokens, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgenerate_kwargs),\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28mrange\u001b[39m(max_tokens),\n\u001b[1;32m    112\u001b[0m ):\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39meos_token_id:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/hindsight_server/lib/python3.10/site-packages/outlines/models/mlxlm.py:181\u001b[0m, in \u001b[0;36mMLXLM.generate_step\u001b[0;34m(self, prompt, temp, top_p, sampler, logits_processor)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m logits_processor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# convert to logits_processor 1d expectation, apply, then convert back\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     logits_1d \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 181\u001b[0m     logits_1d \u001b[38;5;241m=\u001b[39m \u001b[43mlogits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerated_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits_1d\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits_1d\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    184\u001b[0m new_token_single, prob \u001b[38;5;241m=\u001b[39m sample(logits)\n",
      "File \u001b[0;32m~/miniconda3/envs/hindsight_server/lib/python3.10/site-packages/outlines/processors/base_logits_processor.py:68\u001b[0m, in \u001b[0;36mBaseLogitsProcessor.__call__\u001b[0;34m(self, input_ids, logits)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmlx\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmx\u001b[39;00m\n\u001b[1;32m     67\u001b[0m torch_logits \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_dlpack(logits)\n\u001b[0;32m---> 68\u001b[0m processed_torch_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# numpy doesn't support bfloat16, mlx doesn't support direct conversion from torch\u001b[39;00m\n\u001b[1;32m     71\u001b[0m logits_float32_numpy \u001b[38;5;241m=\u001b[39m processed_torch_logits\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/miniconda3/envs/hindsight_server/lib/python3.10/site-packages/outlines/processors/structured.py:90\u001b[0m, in \u001b[0;36mFSMLogitsProcessor.process_logits\u001b[0;34m(self, input_ids, logits)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_first_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 90\u001b[0m     last_token \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fsm_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfsm\u001b[38;5;241m.\u001b[39mget_next_state(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fsm_state, last_token)\n\u001b[1;32m     93\u001b[0m allowed_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfsm\u001b[38;5;241m.\u001b[39mget_next_instruction(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fsm_state)\u001b[38;5;241m.\u001b[39mtokens\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "output = generator(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': '', 'highlights': '', 'quality': 0.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"You are an assistant who generates search queries for an embedding search database to help a user find content of interest.\n",
    "\n",
    "    Below is content that has been on the user's screen recently:\n",
    "\n",
    "    Lock this man up immediately\n",
    "Safest lock ever\n",
    "GTA me trying to get into Los Santos Customs in\n",
    "That BMW driver needs to be locked up for a long time\n",
    "Grey SUV was not dealing with that\n",
    "my uber driver whenever I want to be left alone in silence\n",
    "We got extendo midgets before GTA 6 non aesthetic things Before and after limb lengthening @Pict... • 2d\n",
    "SOMEONE HOPPED IN THE DRIVERS SEAT OF KAI CENAT U-HAUL AND DESTROYED HIS WHOLE SET UP GE\n",
    "I'm glad my taxi driver has already been given the finger 3 times in 5 minutes\n",
    "I'm glad my taxi driver has already been given the finger 3 times in 5 minutes 02\n",
    "\n",
    "    Task:\n",
    "        - Analyze the content above.\n",
    "        - Identify the main themes and topics.\n",
    "        - Generate a single, concise sentence that summarizes the user's interests.\n",
    "        - Include as many relevant keywords and concepts from the content as possible.\n",
    "        - The sentence should be suitable as a search query for finding similar content. \n",
    "\n",
    "    Return a JSON object with the following structure:\n",
    "    {\n",
    "        \"query\": \"sentence for querying the embedding search database\",\n",
    "        \"highlights\": \"Quick list of highlights used to generate the query\",\n",
    "        \"quality\": \"float between 0 and 1 estimating the quality of the generated query for finding content of interest\"\n",
    "    }\n",
    "\n",
    "    Rules:\n",
    "    - Do not add backticks to the JSON eg \\`\\`\\`json\\`\\`\\` is WRONG\n",
    "    - DO NOT RETURN ANYTHING BUT JSON. NO COMMENTS BELOW THE JSON.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hindsight_server",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
